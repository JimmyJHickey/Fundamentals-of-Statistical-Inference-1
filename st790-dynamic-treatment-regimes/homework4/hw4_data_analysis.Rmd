---
title: "Data Analysis Homework 4"
author: "Jimmy Hickey"
date: "11/10/2020"
output: pdf_document
---

# 1

```{r}

knitr::opts_chunk$set(echo = TRUE)
library(DynTxRegime)
library(rgenoud)
library(modelObj)
library(rpart)
library(xtable)

```


## a

```{r}
ldl = read.table("LDL.dat.txt", header=FALSE)
# remove ID column
ldl = ldl[,-1]
names(ldl) = c("L1", "A1", "L2", "S2", "A2", "L3",
"S3", "A3", "L4", "S4", "A4", "Y", "S5")


fSet4 = function(data){
           subsets = list(list("S41",0L),
                           list("S42",c(0L,1L)))

           txOpts = rep(x = 'S42', times = nrow(x = data))
           txOpts[data$S4 == 1L] = "S41"

           return(list("subsets" = subsets, "txOpts" = txOpts))
}

moPropen_S41 = buildModelObjSubset(model = ~ 1,
                              solver.method = 'glm',
                              solver.args = list("family"='binomial'),
                              subset = 'S41', 
                              dp = 4L,
                              predict.method = 'predict.glm',
                              predict.args = list("type"='response'))
moPropen_S42 = buildModelObjSubset(model = ~ L4,
                              solver.method = 'glm',
                              solver.args = list("family"='binomial'),
                              subset = 'S42',
                              dp = 4L,
                              predict.method = 'predict.glm',
                              predict.args = list("type"='response'))


bowl4 = bowl(moPropen = list(moPropen_S41, moPropen_S42),
                           data = ldl, 
                           txName = 'A4',
                           regime = ~ L4,
                           response = -ldl$Y, 
                           BOWLObj = NULL,
                           lambdas = 0.01,
                           kernel = 'linear',
                           kparam = NULL,
                           fSet = fSet4,
                           surrogate = 'sqhinge',
                           verbose = 0L)
####################
# Decision 3
##################


fSet3 = function(data){
           subsets = list(list("S31",0L),
                           list("S32",c(0L,1L)))

           txOpts = rep(x = 'S32', times = nrow(x = data))
           txOpts[data$S3 == 1L] = "S31"

           return(list("subsets" = subsets, "txOpts" = txOpts))
}

moPropen_S31 = buildModelObjSubset(model = ~ 1,
                              solver.method = 'glm',
                              solver.args = list("family"='binomial'),
                              subset = 'S31', 
                              dp = 3L,
                              predict.method = 'predict.glm',
                              predict.args = list("type"='response'))
moPropen_S32 = buildModelObjSubset(model = ~ L3,
                              solver.method = 'glm',
                              solver.args = list("family"='binomial'),
                              subset = 'S32', 
                              dp = 3L,
                              predict.method = 'predict.glm',
                              predict.args = list("type"='response'))


bowl3 = bowl(moPropen = list(moPropen_S31, moPropen_S32),
                           data = ldl, 
                           txName = 'A3',
                           regime = ~ L3,
                           response = rep(0,nrow(ldl)), 
                           BOWLObj = bowl4,
                           lambdas = 0.01,
                           kernel = 'linear',
                           kparam = NULL,
                           fSet = fSet3,
                           surrogate = 'sqhinge',
                           verbose = FALSE)

#################
# decision 2
################


fSet2 = function(data){
           subsets = list(list("S21",0L),
                           list("S22",c(0L,1L)))

           txOpts = rep(x = 'S22', times = nrow(x = data))
           txOpts[data$S2 == 1L] = "S21"

           return(list("subsets" = subsets, "txOpts" = txOpts))
}

moPropen_S21 = buildModelObjSubset(model = ~ 1,
                              solver.method = 'glm',
                              solver.args = list("family"='binomial'),
                              subset = 'S21', 
                              dp = 2L,
                              predict.method = 'predict.glm',
                              predict.args = list(type='response'))

moPropen_S22 = buildModelObjSubset(model = ~ L2,
                              solver.method = 'glm',
                              solver.args = list("family"='binomial'),
                              subset = 'S22', 
                              dp = 2L,
                              predict.method = 'predict.glm',
                              predict.args = list(type='response'))


bowl2 = bowl(moPropen = list(moPropen_S21, moPropen_S22),
                           data = ldl, 
                           txName = 'A2',
                           regime = ~ L2,
                           response = rep(0,nrow(ldl)), 
                           BOWLObj = bowl3,
                           lambdas = 0.01,
                           kernel = 'linear',
                           kparam = NULL,
                           fSet = fSet2,
                           surrogate = 'sqhinge',
                           verbose = FALSE)


##############
# decision 1
###############

fSet1 = function(data){
           subsets = list(list("S1",c(0L,1L)))
           txOpts = rep(x = 'S1', times = nrow(x = data))
           return(list("subsets" = subsets, "txOpts" = txOpts))
}

moPropen_S1 = buildModelObjSubset(model = ~  L1,
                              solver.method = 'glm',
                              solver.args = list("family"='binomial'),
                              subset = 'S1', 
                              dp = 1L,
                              predict.method = 'predict.glm',
                              predict.args = list(type='response'))


bowl1 = bowl(moPropen = moPropen_S1,
                           data = ldl, 
                           txName = 'A1',
                           regime = ~ L1,
                           response = rep(0,nrow(ldl)), 
                           BOWLObj = bowl2,
                           lambdas = 0.01,
                           kernel = 'linear',
                           kparam = NULL,
                           fSet = fSet1,
                           surrogate = 'sqhinge',
                           verbose = FALSE)

```


## b

```{r}
-regimeCoef(bowl1)[1] / regimeCoef(bowl1)[2]

-regimeCoef(bowl2)[1] / regimeCoef(bowl2)[2]

-regimeCoef(bowl3)[1] / regimeCoef(bowl3)[2]

-regimeCoef(bowl4)[1] / regimeCoef(bowl4)[2]


estimator(bowl1)
```

This gives the following treatment rules.

\begin{align*}
\widehat d^{opt}_1 & I( L1 M 177.5022) \\
\widehat d^{opt}_2 & = I(L2 < 206.0815) I(S2 = 0) \\
\widehat d^{opt}_3 & = I(L3 > 96.11033) I(S3 =0) \\
\widehat d^{opt}_4 &= I(L4 < 296.163) I(S4 =  0)
\end{align*}

Here we get a value $\widehat {\mathcal V}(d^{opt}) = 96.17531$. This is smaller than 103.6736 that I got from  question 2 of homework 3. 



\newpage

# 2

```{r}
smart = read.table("SMART.dat.txt", header=FALSE)
names(smart) = c("X11", "X12", "X13", "A1",
                 "X21", "X22", "R2", "A2", "Y")

n = nrow(smart)

# e1, trt 0 -> trt 0
cd1 = (smart$A1 == 0 ) * (smart$A2 == 0)
# e2 trt 0 -> trt 0 or trt 1
cd2 = (smart$A1 == 0 ) * (smart$A2 != smart$R2)
# e3 trt1 -> trt 1
cd3 = (smart$A1 == 1 ) * (smart$A2 == 1)
# e4 trt1 -> trt 0 or trt 1
cd4 = (smart$A1 == 1 ) * (smart$A2 == smart$R2)

```

## a
### i
```{r}

aipw = function(data, cd){
  W = 4 / (data$R2 + 1)
  # from part b of analytical 

  est = sum( cd * W * data$Y) / sum(cd * W) 
  
  # from part c of analytical 

  var = mean( cd * W^2 * (data$Y - est)^2 )

  se = sqrt(1/nrow(data) * var) 
  return( list(est = est, se = se))
}



aipw_r1 = aipw(smart, cd1)
aipw_r2 = aipw(smart, cd2)
aipw_r3 = aipw(smart, cd3)
aipw_r4 = aipw(smart, cd4)


```


### ii
```{r}
library(geepack)

# weighted least squares fit
WLS = function(data, cd){
  W = 4 / (data$R2 + 1)

  # from problem  
  wlsfit = geese(Y ~ cd, family=gaussian, data=data, weights=W,
                corstr="independence",id=1:nrow(data))
  
  # sum for alpha_0 + alpha_1
  est = sum(wlsfit$beta)
  
  # lambda^T * Var(beta) * lambda
  se = sqrt(c(1,1) %*% wlsfit$vbeta %*% c(1,1))
  
  return(list(est = est, se = se))
}

wls_r1 = WLS(smart, cd1)
wls_r2 = WLS(smart, cd2)
wls_r3 = WLS(smart, cd3)
wls_r4 = WLS(smart, cd4)

```



### iii

```{r}
# aipw with estimated propensities
aipw_est = function(data, cd){
  
    prop1 = predict(glm(A1~1,
                      data = data,
                      family = "binomial"),
                      type = "response")[1]

  # 0 -> 0
  prop20 =  predict(glm(A2~1,
                    data=data[(data$A1==0)&(data$R2==0),],
                    family='binomial'),
                    type='response')[1]
  # 1 -> 0
  prop21 =  predict(glm(A2~1,
                    data=data[(data$A1==1)&(data$R2==0),],
                    family='binomial'),
                    type='response')[1]

  
  omega1 = prop1 * data$A1 + (1-prop1)*(1-data$A1)
  omega2 = data$A1 * 
            (data$R2* data$A2 + 
            (1 - data$R2)*(prop21*data$A2 + (1-prop21)*(1-data$A2))) +
          (1 - data$A1) * 
            (data$R2 * (1-data$A2) +
                (1 - data$R2) * (prop20 * data$A2 + (1-prop20)*(1-data$A2)) )
  W = 1/(omega1 * omega2)
  
  est = sum( cd * W * data$Y) / sum(cd * W) 
  
  # from part c of analytical 

  var = mean( cd * W^2 * (data$Y - est)^2 )

  se = sqrt( 1 / nrow(data) * var) 
  return( list(est = est, se = se))
}


aipw_est_r1 = aipw_est(smart, cd1)
aipw_est_r2 = aipw_est(smart, cd2)
aipw_est_r3 = aipw_est(smart, cd3)
aipw_est_r4 = aipw_est(smart, cd4)


a_df = data.frame( rbind(c(1, aipw_r1, wls_r1, aipw_est_r1),
                   c(2, aipw_r2, wls_r2, aipw_est_r2),
                   c(3, aipw_r3, wls_r3, aipw_est_r3),
                   c(4, aipw_r4, wls_r4, aipw_est_r4)))
  
  
names(a_df) = c("regime",
                  "aipw_est",
                  "aipw_se",
                  "wls_est",
                  "wls_se",
                  "aipw2_est",
                  "aip2_se")

```


\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 regime & est 1 & sd 1 & est 2 & sd 2 & est 3 & sd 3 \\
  \hline
  1 & 83.86 & 1.23 & 83.86 & 1.18 & 84.03 & 1.189125 \\
  2 & 79.77 & 1.27 & 79.77 & 1.13 & 79.72 & 1.114411 \\
  3 & 79.44 & 1.08 & 79.44 & 1.16 & 79.38 & 1.155281 \\
  4 & 80.34 & 1.34 & 80.34 & 1.48 & 80.41 & 1.488151 \\
   \hline
\end{tabular}
\end{table}

Notice that the first estimates are the same as those from the second method. This matches the theory proven in the analytical section. Also, as expected, they have different standard errors. 


## b

```{r}
calc_p = function(est_se1, est_se2){
  variance = est_se1$se^2+est_se2$se^2
  
  test_stat = (est_se1$est-est_se2$est)/sqrt(variance)
  
  pval = 2 * (1 - pnorm(abs(test_stat)))
  
  return(list(test_stat, pval))
  
}



calc_p(aipw_r1, aipw_r3)
calc_p(wls_r1, wls_r3)
calc_p(aipw_est_r1, aipw_est_r3)

```

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 method & test stat & pval  \\
  \hline
    1 & 2.698345 & 0.00697   \\
    2 & 2.669333 & 0.00760  \\
    3 & 2.803768 & 0.00505 \\
   \hline
\end{tabular}
\end{table}




In all cases we reject the null hypothesis at the level of 0.05. We are 95% confident that the value of the last burdensome regime is different from the most burdensome.

(Notice that the last test statistic is very large. This is again due to the very small standard errors.)


## c

```{r}

calc_p = function(est_se1, est_se2, data, cd1, cd2){
  W = 4 / (data$R2 + 1)
  
  var = est_se1$se^2 + est_se2$se^2 - 
    2* mean(cd1 * W * cd2 * W * 
          (data$Y - est_se1$est) * (data$Y - est_se2$est)) / nrow(data)
  
  test_stat = (est_se1$est - est_se2$est)/sqrt(var)
  
  pval = 2 * (1 - pnorm(abs(test_stat)))
  
  return(c(test_stat, pval))
  
}

calc_p(aipw_r1, aipw_r3, smart, cd1, cd2)
calc_p(wls_r1, wls_r3, smart, cd1, cd2)
calc_p(aipw_est_r1, aipw_est_r3, smart, cd1, cd2)


```

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 method & test stat & pval  \\
  \hline
    1 & 2.93643 & 0.00332   \\
    2 & 2.89915 & 0.00374  \\
    3 & 3.04372 & 0.00233  \\
   \hline
\end{tabular}
\end{table}



In all cases we reject the null hypothesis at the level of 0.05. We are 95% confident that the value of the regime starting with trt 0 that switches nonresponders to trt 1 is different than keeping nonresponders on trt 0.



## d
### i
```{r}
###################
# decision 2
###################




fSet2 <- function(data){
  subsets <- list(list("S20",c(0L)),
                  list("S21",c(1L)),
                  list("S22",c(0L,1L)))
                  
  txOpts <- rep(x = 'S22', times = nrow(x = data))
  txOpts[data$A1 == 0L & data$R2 == 1L] <- "S20"
  txOpts[data$A1 == 1L & data$R2 == 1L] <- "S21"
  
  return(list("subsets" = subsets, "txOpts" = txOpts))
}

moCont_S22 = buildModelObjSubset(model = ~ A1,
                                 solver.method = "lm",
                                 subset= "S22",
                                 dp = 2L)


moMain_S22 = buildModelObjSubset(model = ~ A1,
                                 solver.method = "lm",
                                 subset= "S22",
                                 dp = 2L)

# remember negative response to minimize
q2 = qLearn(moMain = moMain_S22,
            moCont = moCont_S22,
            data = smart,
            response = -smart$Y,
            txName = 'A2',
            fSet = fSet2)


##################
# decision 1
#################

moMain_1 = buildModelObj(model = ~1,
                         solver.method = 'lm')


moCont_1 = buildModelObj(model = ~1,
                         solver.method = 'lm')


q1 = qLearn(moMain = moMain_1,
            moCont = moCont_1,
            data = smart,
            response = q2,
            txName = "A1",
            fSet = NULL)

```

We can now use our $Q$ functions and estimates to estimate our optimal regime.

\begin{align*}
d_1^{opt} & = I(\beta_{12} > 0) \\
\widehat d_1^{opt} & = I(0.3339) \\
  & = 1\\ \\
d_2^{opt} & = I(\beta_{23 } + \beta_24 a_1 > 0) \\
  & = I(7.123 - 5.399 a_1 > 0) \\
  & = I(a_1 < 1.32) \\
  & = 1
\end{align*}

Thus our optimal regime is (1,1) with value $79.38164$.


### ii


```{r}
moPropen_2 = buildModelObjSubset(model = ~ 1,
                solver.method = 'glm',
                solver.args = list("family"='binomial'),
                subset = 'S22',
                dp = 2L,
                predict.method = 'predict.glm',
                predict.args = list(type='response'))



bowl2 = bowl(moPropen = moPropen_2,
              data = smart,
              txName = 'A2',
              regime = ~ A1,
              response = as.vector(x = -smart$Y),
              BOWLObj = NULL,
              lambdas = 10.0^{seq(from = -4, to = -1, by = 1)},
              cvFolds = 10L,
              kernel = 'linear',
              kparam = NULL,
              fSet = fSet2,
              surrogate = 'sqhinge',
              verbose = 0L)



fSet1 = function(data){
  subsets = list(list("S1", c(0, 1)))

  txOpts = rep("S1", nrow(data))
  
  return(list("subsets" = subsets, "txOpts" = txOpts))  
}

moPropen_1 = buildModelObjSubset(model = ~ 1,
              solver.method = 'glm',
              solver.args = list("family"='binomial'),
              subset = 'S1',
              dp = 1L,
              predict.method = 'predict.glm',
              predict.args = list(type='response'))



bowl1 = bowl(moPropen = moPropen_1,
             data = smart,
            txName = 'A1',
            regime = ~ 1,
            response = rep(0,nrow(smart)),
            BOWLObj = bowl2,
            lambdas = 0.01,
            kernel = 'linear',
            kparam = NULL,
            fSet = fSet1,
            surrogate = 'sqhinge',
            verbose = FALSE)


regimeCoef(bowl2)
regimeCoef(bowl1)
estimator(bowl1)

```
We can use our given decision functions with our estimates to estimate an optmial regime.

\begin{align*}
d_1^{opt} & = I(\eta_{11} > 0) \\
\widehat d_1^{opt} &= I(0.01301 > 0) \\
  & = 1 \\ \\
d_2^{opt} & = I(\eta_{21} + \eta_{22} a_1 > 0) \\
  & = I(0.02519 + 0.0051 a_1 > 0) \\
  & = I(a_1 > -4.9392) \\
  & = 1
\end{align*}

Again, we get an optimal treatment of (1,1). but now with a smaller value of 78.42707.

Notice that the values from part (d) are similar to part (a).

## e

### i
```{r}
#################
# decision 2
#################
moMain_S22 = buildModelObjSubset(model = ~ X11 + X12 + X13 + A1 + X21 + X22,
                              solver.method = 'lm',
                              subset = "S22",
                              dp = 2L,
                              predict.method = 'predict.lm')

moCont_S22 = buildModelObjSubset(model = ~ X11 + X12 + X13 + A1 + X21 + X22,
                              solver.method = 'lm',
                              subset = "S22",
                              dp = 2L,
                              predict.method = 'predict.lm')


q2 = qLearn(moMain = moMain_S22,
            moCont = moCont_S22,
            data = smart,
            response = - smart$Y,
            txName = 'A2',
            fSet = fSet2,
            verbose = FALSE)

###################
# decision 1
##################

moMain_1 = buildModelObj(model = ~ X11 + X12 + X13,
                            solver.method = 'lm',
                            predict.method = 'predict.lm')

moCont_1 = buildModelObj(model = ~ X11 + X12 + X13,
                            solver.method = 'lm',
                            predict.method = 'predict.lm')

q1 = qLearn(moMain = moMain_1,
            moCont = moCont_1,
            data = smart,
            response = q2,
            txName = 'A2',
            fSet = NULL,
            verbose = FALSE)



coef(q2)
coef(q1)
estimator(q1)
```

We can now construct our optimal regime.

\begin{align*}
d_1^{opt} & = I(\beta_{12} \widetilde h_1>0) \\
\widehat d_1^{opt} & = I(3.1696 X11 - 0.0678 X12 + 0.09539 X13 > 0) \\ \\
d_2^{opt} & = I(\beta_{22} \widetilde h_2>0) \\
\widehat d_2^{opt} & = I(0.13420 X11 - 0.17948 X12 -0.094825 X13 - 4.31633 A1 + 0.0008 X21 + 3.10707 X22> 0) \\ \\
\end{align*}


With an estiamted value of 78.37626.


### ii
```{r}
bowl2 = bowl(moPropen = moPropen_2,
             data = smart,
             txName = 'A2',
             regime = ~ X11 + X12 + X13 + A1 + X21 + X22,
             response = -smart$Y,
             BOWLObj = NULL,
             lambdas = 0.01,
             kernel = 'linear',
             kparam = NULL,
             fSet = fSet2,
             surrogate = 'sqhinge',
             verbose = FALSE)

bowl1 = bowl(moPropen = moPropen_1,
             data = smart,
             txName = 'A1',
             regime = ~ X11 + X12 + X13,
             response = rep(0,n),
             BOWLObj = bowl2,
             lambdas = 0.01,
             kernel = 'linear',
             kparam = NULL,
             fSet = fSet1,
             surrogate = 'sqhinge',
             verbose = FALSE)

regimeCoef(bowl1)
regimeCoef(bowl2)
estimator(bowl2)
estimator(bowl1)
```

We can now construct our optimal regime.

\begin{align*}
d_1^{opt} & = I(\\eta_{11}^T \widetilde h_1>0) \\
\widehat d_1^{opt} & = I(1.89745- 0.09431 X11 - 0.04903 X12 + 0.00669 X13 > 0) \\ \\
d_2^{opt} & = I(\eta_{21}^T \widetilde h_2>0) \\
\widehat d_2^{opt} & = I(-2.1139 + 0.3386 X11 + 0.00974 X12 + 0.309718 X13 - 0.019804 A1 - 0.012952 X21 - 0.23999 X22> 0) \\ \\
\end{align*}


With a value of 54.06177. This may be because of the sensitivity to propensity models. Also there are few individuals.

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline 
  & Recommended Treatments & \\
  \hline
 0 & 1 & NA  \\
  \hline
   36 & 62 & 52
\end{tabular}
\end{table}


## f

### i

```{r}
regime1 = function(eta1, data){
  return( as.integer(data$X13 > eta1))
}


regime2 = function(eta2, data){
  return( (data$X21 > eta2) *
          (data$R2 == 0) +
            data$A1 * (data$R2 == 1))
}


domains = rbind(c(min(smart$X13)-1, max(smart$X13)+1),
                  c(min(smart$X21)-1, max(smart$X21)+1))
val0 = c(mean(smart$X13), mean(smart$X21))


IPW = optimalSeq(moPropen = list(moPropen_1, moPropen_2),
                 data = smart,
                 response = -smart$Y,
                 txName = c("A1", 'A2'),
                 fSet = list(fSet1, fSet2),
                 regimes = list(regime1, regime2),
                 Domains = domains,
                 pop.size = 1000L,
                 starting.values = val0,
                 verbose = FALSE)
  


regimeCoef(IPW)
estimator(IPW)

```

With these $\eta$'s, we can formulate our estimated decision rules.

\begin{align*}
d_1(h_1; \eta_1) & = I(X13 > \eta_1) \\
\widehat d_1(h_1;\widehat  \eta_1) & = I(X13 > 94.55671) \\ \\
d_2(d_2; \eta_2) & = I(X13 >  \eta_2) I(R2 = 0) + A1 I(R2 = 1) \\
\widehat d_2(d_2; \widehat \eta_2) & = I(X13 >  97.38361) I(R2 = 0) + A1 I(R2 = 1) 
\end{align*}


With a value of 58.3431. This value may be small for the same reasons as before.


### ii

```{r}

moMain_2 = buildModelObj(model = ~ A1,
              solver.method = 'lm',
              predict.method = 'predict.lm')

moCont_2 = buildModelObj(model = ~ A1,
              solver.method = 'lm',
              predict.method = 'predict.lm')


AIPW = optimalSeq(moMain = list(moMain_1, moMain_2),
                 moCont = list(moCont_1, moCont_2),
                 moPropen = list(moPropen_1, moPropen_2),
                 data = smart,
                 response = -smart$Y,
                 txName = c("A1", 'A2'),
                 fSet = list(fSet1, fSet2),
                 regimes = list(regime1, regime2),
                 Domains = domains,
                 pop.size = 1000L,
                 starting.values = val0,
                 verbose = FALSE)

regimeCoef(AIPW)
estimator(AIPW)
```


With these $\eta$'s, we can formulate our estimated decision rules.

\begin{align*}
d_1(h_1; \eta_1) & = I(X13 > \eta_1) \\
\widehat d_1(h_1;\widehat  \eta_1) & = I(X13 > 97.86032) \\ \\
d_2(d_2; \eta_2) & = I(X13 >  \eta_2) I(R2 = 0) + A1 I(R2 = 1) \\
\widehat d_2(d_2; \widehat \eta_2) & = I(X13 >  82.38203) I(R2 = 0) + A1 I(R2 = 1) 
\end{align*}


With a value of 77.89197.


The AIPW estimate is similar to that of qLearning while the IPW estimate is closer to the BOWL estimate.